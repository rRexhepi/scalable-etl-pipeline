# scalable-etl-pipeline
Design and implement a scalable ETL pipeline capable of handling large datasets efficiently. The pipeline will extract data from various sources (APIs or databases), transform it using Pandas or PySpark, and load the transformed data into an open-source data warehouse. The entire workflow will be orchestrated and scheduled using Apache Airflow.
